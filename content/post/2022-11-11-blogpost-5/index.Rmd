---
title: "Blogpost 5"
author: "Arlo Ramoutar, Cleo Ulisse, Tessa Wu, Bridget Zhang"
date: "2022-11-11"
slug: []
categories: []
tags: []
description: Combining Data
toc: yes
authors: Arlo Ramoutar, Cleo Ulisse, Tessa Wu, Bridget Zhang
series: []
lastmod: "2022-11-11T09:42:56-05:00"
featuredVideo: null
featuredImage: null
---

We decided to include information from the US Census demographic data from 2017 that we found on kaggle. 
This dataset contains demographic data of the U.S. population from 2017, with 37 columns and 74000 rows. By combining this dataset with out schools scores data, we can use this information to find other factors that might contribute to deviations in SAT scores, e.g. gender ratios, racial demographics.

Firstly, we loaded and cleaned the data. We used the load.clean.data.csv file. 
`[load_and_clean_data.R](/load_and_clean_data.R)`
One comparison to note is that the State.Name column in the original dataset is equal to the State column in the new dataset.

```{r, message=FALSE}

library(tidyverse)
school_scores_data_clean <- read_csv("school_scores_data_clean.csv")

colnames(school_scores_data_clean) <- c('year', 'state.code', 'state.name', 'total.math', 'total.testtakers', 'total.verbal', 'arts.music.avggpa', 'arts.music.avgyears', 'english.avggpa', 'english.avgyears', 'foreign.languages.avggpa', 'foreign.languages.avgyears', 'mathematics.avggpa', 'mathematics.avgyears', 'natural.sciences.avggpa', 'natural.sciences.avgyears', 'social.sciences.history.avggpa', 'social.sciences.history.avgyears', 'income.between.20.40k.math', 'income.between.20.40k.testtakers','income.between.20.40k.verbal', 'income.between.40.60k.math', 'income.between.40.60k.testtakers', 'income.between.40.60k.verbal', 'income.between.60.80k.math', 'income.between.60.80k.testtakers','income.between.60.80k.verbal', 'income.between.80.100k.math','income.between.80.100k.testtakers', 'income.between.80.100k.verbal', 'income.less.than.20k.math', 'income.less.than.20k.testtakers', 'income.less.than.20k.verbal', 'income.more.than.100k.math', 'income.more.than.100k.testtakers', 'income.more.than.100k.verbal', 'gpa.aminus.math', 'gpa.aminus.testtakers', 'gpa.aminus.verbal', 'gpa.aplus.math', 'gpa.aplus.testtakers', 'gpa.aplus.verbal', 'gpa.a.math', 'gpa.a.testtakers', 'gpa.a.verbal', 'gpa.b.math', 'gpa.b.testtakers', 'gpa.b.verbal', 'gpa.c.math', 'gpa.c.testtakers', 'gpa.c.verbal', 'gpa.d.or.lower.math', 'gpa.d.or.lower.testtakers', 'gpa.d.or.lower.verbal', 'gpa.no.response.math', 'gpa.no.response.testtakers', 'gpa.no.response.verbal', 'gender.female.math','gender.female.testtakers', 'gender.female.verbal', 'gender.male.math', 'gender.male.testtakers', 'gender.male.verbal', 'score.between.200.to.300.math.females', 'score.between.200.to.300.math.males', 'score.between.200.to.300.math.total', 'score.between.200.to.300.verbal.females', 'score.between.200.to.300.verbal.males', 'score.between.200.to.300.verbal.total', 'score.between.300.to.400.math.females', 'score.between.300.to.400.math.males', 'score.between.300.to.400.math.total', 'score.between.300.to.400.verbal.females', 'score.between.300.to.400.verbal.males', 'score.between.300.to.400.verbal.total', 'score.between.400.to.500.math.females', 'score.between.400.to.500.math.males', 'score.between.400.to.500.math.total', 'score.between.400.to.500.verbal.females', 'score.between.400.to.500.verbal.males', 'score.between.400.to.500.verbal.total', 'score.between.500.to.600.math.females', 'score.between.500.to.600.math.males', 'score.between.500.to.600.math.total', 'score.between.500.to.600.verbal.females', 'score.between.500.to.600.verbal.males', 'score.between.500.to.600.verbal.total', 'score.between.600.to.700.math.females', 'score.between.600.to.700.math.males', 'score.between.600.to.700.math.total', 'score.between.600.to.700.verbal.females', 'score.between.600.to.700.verbal.males', 'score.between.600.to.700.verbal.total', 'score.between.700.to.800.math.females', 'score.between.700.to.800.math.males', 'score.between.700.to.800.math.total', 'score.between.700.to.800.verbal.females', 'score.between.700.to.800.verbal.males', 'score.between.700.to.800.verbal.total')

newdata <- school_scores_data_clean %>% mutate(total.score = total.math + total.verbal) %>% 
  mutate(income.between.20.40k = income.between.20.40k.math + income.between.20.40k.verbal) %>%
  mutate(income.between.40.60k = income.between.40.60k.math + income.between.40.60k.verbal) %>%
  mutate(income.between.60.80k = income.between.60.80k.math + income.between.60.80k.verbal) %>%
  mutate(income.between.80.100k = income.between.80.100k.math + income.between.80.100k.verbal) %>%
  mutate(income.less.than.20k = income.less.than.20k.math + income.less.than.20k.verbal) %>%
  mutate(income.more.than.100k = income.more.than.100k.math + income.more.than.100k.verbal)
  
```

Now we load in the new dataset:

```{r}
library(tidyverse)
census_data <- read_csv("acs2017_census_tract_data.csv")

censusdata <- census_data %>% mutate(State.Name = State) %>% ungroup() %>%
  select(State.Name, White, Black, Asian, Native, Pacific)

newdata_1 <- school_scores_data_clean %>% mutate(total.score = total.math + total.verbal) %>% 
  mutate(income.between.20.40k = income.between.20.40k.math + income.between.20.40k.verbal) %>%
  mutate(income.between.40.60k = income.between.40.60k.math + income.between.40.60k.verbal) %>%
  mutate(income.between.60.80k = income.between.60.80k.math + income.between.60.80k.verbal) %>%
  mutate(income.between.80.100k = income.between.80.100k.math + income.between.80.100k.verbal) %>%
  mutate(income.less.than.20k = income.less.than.20k.math + income.less.than.20k.verbal) %>%
  mutate(income.more.than.100k = income.more.than.100k.math + income.more.than.100k.verbal) %>%
  ungroup()

incomedata <- newdata_1 %>% select(state.name, total.score, income.between.20.40k, income.between.40.60k,
                                   income.between.60.80k, income.between.80.100k, income.less.than.20k,
                                   income.more.than.100k)


newdata_inc <- incomedata %>% crossing(censusdata)
newdata_inc
```

A new dataset we found that we would like to combine without original data is this https://www.kaggle.com/datasets/muonneutrino/us-census-demographic-data, which contains demographic data of the U.S. population from 2017 with 37 columns and 740000 rows total. We chose this dataset to combine with our original one because we want to see if we can find other factors that could contribute to or explain the differences in SAT scores between states and different income levels â€” for example, certain states may have racial and gender demographics that historically perform more poorly on standardized tests. 

We hope to combine them by using the merge() function since both datasets have a column dedicated to the 50 states, but we are concerned about the fact that this new dataset has 74000 rows, whereas our original one only has 578. To overcome this we may need to create new columns in the census dataset.


